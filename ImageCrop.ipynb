{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ/+/vSYW7eus9E8vJ6lkN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmaadaziz/ai-face-crop/blob/main/ImageCrop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3KLBC5R2cGG",
        "outputId": "fd5eaa87-fce2-4ac0-a15f-aa98ce4aabbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->mtcnn) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=2.0.0->mtcnn) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.0.0->mtcnn) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.0.0->mtcnn) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->mtcnn) (0.1.2)\n",
            "Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python mtcnn numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from mtcnn import MTCNN"
      ],
      "metadata": {
        "id": "rH4VRaX53EuO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_gamma(image, gamma=1.0):\n",
        "    inv_gamma = 1.0 / gamma\n",
        "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "    return cv2.LUT(image, table)"
      ],
      "metadata": {
        "id": "4-VA0GJg3I55"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to resize an image while maintaining the aspect ratio\n",
        "def resize_with_aspect_ratio(image, target_width=None, target_height=None):\n",
        "    (h, w) = image.shape[:2]\n",
        "    if target_width is None and target_height is None:\n",
        "        return image\n",
        "\n",
        "    if target_width is None:\n",
        "        r = target_height / float(h)\n",
        "        dim = (int(w * r), target_height)\n",
        "    else:\n",
        "        r = target_width / float(w)\n",
        "        dim = (target_width, int(h * r))\n",
        "\n",
        "    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
        "    return resized"
      ],
      "metadata": {
        "id": "DaHlZz1m3N1h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add padding around the image\n",
        "def add_padding(image, padding_ratio=0.1):\n",
        "    (h, w) = image.shape[:2]\n",
        "    padding_h = int(h * padding_ratio)\n",
        "    padding_w = int(w * padding_ratio)\n",
        "\n",
        "    padded_image = cv2.copyMakeBorder(\n",
        "        image, padding_h, padding_h, padding_w, padding_w, cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
        "    )\n",
        "    return padded_image"
      ],
      "metadata": {
        "id": "Q4jyEX3t3RLe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_face(image_path, output_path, gamma=1.5, target_width=256, padding_ratio=0.5):\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Initialize MTCNN face detector\n",
        "    detector = MTCNN()\n",
        "\n",
        "    # Detect faces in the image\n",
        "    result = detector.detect_faces(img)\n",
        "\n",
        "    if result:\n",
        "        # Extract bounding box from the first detected face\n",
        "        bounding_box = result[0]['box']\n",
        "        x, y, width, height = bounding_box\n",
        "\n",
        "        # Calculate padding\n",
        "        padding_x = int(width * padding_ratio)\n",
        "        padding_y = int(height * padding_ratio)\n",
        "\n",
        "        # Calculate new bounding box with padding\n",
        "        new_x = max(0, x - padding_x)\n",
        "        new_y = max(0, y - padding_y)\n",
        "        new_width = min(img.shape[1] - new_x, width + 2 * padding_x)\n",
        "        new_height = min(img.shape[0] - new_y, height + 2 * padding_y)\n",
        "\n",
        "        # Crop the face from the image with padding\n",
        "        cropped_face = img[new_y:new_y+new_height, new_x:new_x+new_width]\n",
        "\n",
        "        # Apply gamma correction to the cropped face\n",
        "        cropped_face = adjust_gamma(cropped_face, gamma)\n",
        "\n",
        "        # Resize the cropped face while maintaining the aspect ratio\n",
        "        cropped_face = resize_with_aspect_ratio(cropped_face, target_width=target_width)\n",
        "\n",
        "        # Save the final processed image\n",
        "        cv2.imwrite(output_path, cropped_face)\n",
        "        print(f\"Cropped and enhanced face saved to {output_path}\")\n",
        "    else:\n",
        "        print(\"No face detected\")"
      ],
      "metadata": {
        "id": "V9lu4hbM3Td_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_face('./image.jpg', './output_image1.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4FOacze494q",
        "outputId": "35a06c3e-04c8-453b-fd63-e66c2b557ed6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 52 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f77cf613490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
            "Cropped and enhanced face saved to ./output_image1.jpg\n"
          ]
        }
      ]
    }
  ]
}